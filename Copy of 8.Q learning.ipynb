{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1n2-Lzd1KTgT6jZgLm_8S5okt6KhvyVuZ","timestamp":1683709730246}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install cmake 'gym[atari]' scipy\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"slth0Ttfh2Q5","executionInfo":{"status":"ok","timestamp":1683709772255,"user_tz":-330,"elapsed":10230,"user":{"displayName":"Sherin Shibi C","userId":"05670848646754386888"}},"outputId":"d88718fd-f0d3-4848-84dd-ecc5a1105363"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.25.2)\n","Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.10.1)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.22.4)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n","Collecting ale-py~=0.7.5 (from gym[atari])\n","  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (5.12.0)\n","Installing collected packages: ale-py\n","Successfully installed ale-py-0.7.5\n"]}]},{"cell_type":"code","source":["import gym\n","\n","env = gym.make(\"Taxi-v3\").env\n","\n","#env.render()"],"metadata":{"id":"Qk9bb28siK1C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683709773472,"user_tz":-330,"elapsed":1222,"user":{"displayName":"Sherin Shibi C","userId":"05670848646754386888"}},"outputId":"6f22562d-3100-4c9f-bbc0-71d33387a1ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["env.reset() # reset environment to a new, random state\n","#env.render()\n","\n","print(\"Action Space {}\".format(env.action_space))\n","print(\"State Space {}\".format(env.observation_space))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xFhbCK6CiQUx","executionInfo":{"status":"ok","timestamp":1683709774162,"user_tz":-330,"elapsed":695,"user":{"displayName":"Sherin Shibi C","userId":"05670848646754386888"}},"outputId":"40a5ba79-1c6d-4ce6-f50d-3c87bda1f786"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Action Space Discrete(6)\n","State Space Discrete(500)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","q_table = np.zeros([env.observation_space.n, env.action_space.n])"],"metadata":{"id":"Z94SGXYpi1r_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\"\"\"Training the agent\"\"\"\n","\n","import random\n","from IPython.display import clear_output\n","\n","# Hyperparameters\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","\n","# For plotting metrics\n","all_epochs = []\n","all_penalties = []\n","\n","for i in range(1, 100001):\n","    state = env.reset()\n","\n","    epochs, penalties, reward, = 0, 0, 0\n","    done = False\n","    \n","    while not done:\n","        if random.uniform(0, 1) < epsilon:\n","            action = env.action_space.sample() # Explore action space\n","        else:\n","            action = np.argmax(q_table[state]) # Exploit learned values\n","\n","        next_state, reward, done, info = env.step(action) \n","        \n","        old_value = q_table[state, action]\n","        next_max = np.max(q_table[next_state])\n","        \n","        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n","        q_table[state, action] = new_value\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        state = next_state\n","        epochs += 1\n","        \n","    if i % 100 == 0:\n","        clear_output(wait=True)\n","        print(f\"Episode: {i}\")\n","\n","print(\"Training finished.\\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WlfDOmG2i2nY","executionInfo":{"status":"ok","timestamp":1683709874542,"user_tz":-330,"elapsed":100382,"user":{"displayName":"Sherin Shibi C","userId":"05670848646754386888"}},"outputId":"a9e54a73-abf7-4486-af20-d062e12ddc90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 100000\n","Training finished.\n","\n","CPU times: user 1min 32s, sys: 9.63 s, total: 1min 42s\n","Wall time: 1min 40s\n"]}]},{"cell_type":"code","source":["q_table[328]\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i9TIUA0si6Ew","executionInfo":{"status":"ok","timestamp":1683709874543,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sherin Shibi C","userId":"05670848646754386888"}},"outputId":"71a78ca2-f454-4c4b-f572-f973297b5469"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ -2.40598322,  -2.27325184,  -2.41151719,  -2.35971346,\n","       -10.807041  , -11.03794561])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n","\n","total_epochs, total_penalties = 0, 0\n","episodes = 100\n","\n","for _ in range(episodes):\n","    state = env.reset()\n","    epochs, penalties, reward = 0, 0, 0\n","    \n","    done = False\n","    \n","    while not done:\n","        action = np.argmax(q_table[state])\n","        state, reward, done, info = env.step(action)\n","\n","        if reward == -10:\n","            penalties += 1\n","\n","        epochs += 1\n","\n","    total_penalties += penalties\n","    total_epochs += epochs\n","\n","print(f\"Results after {episodes} episodes:\")\n","print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n","print(f\"Average penalties per episode: {total_penalties / episodes}\")"],"metadata":{"id":"4JCQqb8ojlxD","executionInfo":{"status":"ok","timestamp":1683709874543,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sherin Shibi C","userId":"05670848646754386888"}},"outputId":"6916c41f-a9c5-401c-acac-bdfa308d163e","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Results after 100 episodes:\n","Average timesteps per episode: 13.31\n","Average penalties per episode: 0.0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"_GLKvN3hjmIv"},"execution_count":null,"outputs":[]}]}