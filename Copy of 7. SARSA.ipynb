{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1R4LZhGw6LBkN4SIrZAy1fz07eB3KAf96","timestamp":1683707698556}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"vtdD7Pc7a0KM"},"outputs":[],"source":["import numpy as np\n","import gym\n"]},{"cell_type":"code","source":["#Building the environment\n","env = gym.make('FrozenLake-v1')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YR7iBFFmbJ4L","executionInfo":{"status":"ok","timestamp":1681201629147,"user_tz":-330,"elapsed":452,"user":{"displayName":"SIVAPRASATH S (RA2011047010118)","userId":"17997647111988574747"}},"outputId":"551fea37-a8ef-4107-e772-7af0fa848408"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}]},{"cell_type":"code","source":["#Defining the different parameters\n","epsilon = 0.9\n","total_episodes = 10000\n","max_steps = 100\n","alpha = 0.85\n","gamma = 0.95\n","\n","#Initializing the Q-matrix\n","Q = np.zeros((env.observation_space.n, env.action_space.n))\n"],"metadata":{"id":"JsdFZ9esbOEQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Function to choose the next action\n","def choose_action(state):\n","\taction=0\n","\tif np.random.uniform(0, 1) < epsilon:\n","\t\taction = env.action_space.sample()\n","\telse:\n","\t\taction = np.argmax(Q[state, :])\n","\treturn action\n","\n","#Function to learn the Q-value\n","def update(state, state2, reward, action, action2):\n","\tpredict = Q[state, action]\n","\ttarget = reward + gamma * Q[state2, action2]\n","\tQ[state, action] = Q[state, action] + alpha * (target - predict)\n"],"metadata":{"id":"seYwi2cMbSKL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Initializing the reward\n","reward=0\n","\n","# Starting the SARSA learning\n","for episode in range(total_episodes):\n","\tt = 0\n","\tstate1 = env.reset()\n","\taction1 = choose_action(state1)\n","\n","\twhile t < max_steps:\n","\t\t#Visualizing the training\n","\t\t#env.render()\n","\t\t\n","\t\t#Getting the next state\n","\t\tstate2, reward, done, info = env.step(action1)\n","\n","\t\t#Choosing the next action\n","\t\taction2 = choose_action(state2)\n","\t\t\n","\t\t#Learning the Q-value\n","\t\tupdate(state1, state2, reward, action1, action2)\n","\n","\t\tstate1 = state2\n","\t\taction1 = action2\n","\t\t\n","\t\t#Updating the respective vaLues\n","\t\tt += 1\n","\t\treward += 1\n","\t\t\n","\t\t#If at the end of learning process\n","\t\tif done:\n","\t\t\tbreak\n"],"metadata":{"id":"r2FDbXrIbYLV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Evaluating the performance\n","print (\"Performance : \", reward/total_episodes)\n","\n","#Visualizing the Q-matrix\n","print(Q)\n"],"metadata":{"id":"vI63k2-gb7wZ","executionInfo":{"status":"ok","timestamp":1681201824452,"user_tz":-330,"elapsed":332,"user":{"displayName":"SIVAPRASATH S (RA2011047010118)","userId":"17997647111988574747"}},"outputId":"f8a2ba99-23af-40b5-a07e-524945e7e213","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Performance :  0.0001\n","[[7.95389839e-04 1.25723391e-03 1.29172745e-05 4.09553819e-04]\n"," [2.13448087e-03 2.52746677e-03 8.91617004e-04 1.69993677e-03]\n"," [2.34997262e-03 6.32785832e-04 3.07591973e-03 2.52620491e-03]\n"," [5.13500776e-04 3.70320781e-04 2.85944982e-05 5.08817122e-04]\n"," [2.09021009e-01 3.23157829e-03 3.84705568e-02 4.57103456e-03]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [3.03352679e-04 8.56844545e-02 8.70989789e-03 8.59496491e-05]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [3.71408662e-02 9.40489613e-03 7.13654991e-03 5.41660416e-03]\n"," [4.60354872e-02 6.94071970e-02 9.75564269e-03 9.20430897e-07]\n"," [1.66303640e-03 1.95476126e-03 4.15020305e-02 4.61279488e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [5.86491779e-02 4.19860994e-02 8.00700176e-01 1.27468211e-02]\n"," [4.47229344e-02 1.42086008e-02 2.29653724e-01 1.56085570e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"BUV8KPN2bbSA"},"execution_count":null,"outputs":[]}]}